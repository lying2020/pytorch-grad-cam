# PyTorch Grad-CAM åº“åŠŸèƒ½æ€»ç»“

## ğŸ“š åº“çš„ç”¨é€”

`pytorch-grad-cam` æ˜¯ä¸€ä¸ªç”¨äº **PyTorch æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ** çš„é«˜çº§å·¥å…·åº“ã€‚å®ƒå¯ä»¥å¸®åŠ©ä½ ï¼š

1. **å¯è§†åŒ–æ¨¡å‹å†³ç­–è¿‡ç¨‹**ï¼šç†è§£æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­å…³æ³¨å“ªäº›åŒºåŸŸ
2. **è¯Šæ–­æ¨¡å‹è¡Œä¸º**ï¼šå‘ç°æ¨¡å‹çš„æ½œåœ¨é—®é¢˜å’Œåè§
3. **ç ”ç©¶å’Œå¼€å‘**ï¼šä½œä¸ºå¯è§£é‡Šæ€§æ–¹æ³•çš„åŸºå‡†æµ‹è¯•å·¥å…·
4. **ç”Ÿäº§ç¯å¢ƒè°ƒè¯•**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¯Šæ–­æ¨¡å‹é¢„æµ‹

## ğŸ¯ ä¸»è¦åŠŸèƒ½

### 1. å¤šç§ CAM (Class Activation Map) æ–¹æ³•

åº“æä¾›äº† **14ç§** ä¸åŒçš„å¯è§†åŒ–æ–¹æ³•ï¼š

| æ–¹æ³• | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **GradCAM** | ä½¿ç”¨å¹³å‡æ¢¯åº¦åŠ æƒæ¿€æ´»å€¼ | æœ€å¸¸ç”¨ï¼Œå¹³è¡¡é€Ÿåº¦å’Œæ•ˆæœ |
| **HiResCAM** | é€å…ƒç´ ç›¸ä¹˜æ¿€æ´»å€¼å’Œæ¢¯åº¦ | ä¿è¯å¿ å®æ€§çš„ç†è®ºä¿è¯ |
| **GradCAM++** | ä½¿ç”¨äºŒé˜¶æ¢¯åº¦ | æ›´ç²¾ç¡®çš„å®šä½ |
| **ScoreCAM** | é€šè¿‡æ‰°åŠ¨å›¾åƒæµ‹é‡è¾“å‡ºå˜åŒ– | æ— éœ€æ¢¯åº¦ï¼Œæ›´ç¨³å®š |
| **AblationCAM** | é€šè¿‡ç½®é›¶æ¿€æ´»å€¼æµ‹é‡è¾“å‡ºä¸‹é™ | æ¢¯åº¦æ— å…³æ–¹æ³• |
| **XGradCAM** | ç”¨å½’ä¸€åŒ–æ¿€æ´»å€¼ç¼©æ”¾æ¢¯åº¦ | GradCAMçš„æ”¹è¿›ç‰ˆ |
| **EigenCAM** | ä½¿ç”¨æ¿€æ´»å€¼çš„ç¬¬ä¸€ä¸»æˆåˆ† | æ— ç±»åˆ«åŒºåˆ†ï¼Œä½†ç»“æœå¾ˆå¥½ |
| **EigenGradCAM** | æ¿€æ´»å€¼Ã—æ¢¯åº¦çš„ç¬¬ä¸€ä¸»æˆåˆ† | æœ‰ç±»åˆ«åŒºåˆ†ï¼Œæ¯”GradCAMæ›´æ¸…æ™° |
| **LayerCAM** | ç”¨æ­£æ¢¯åº¦ç©ºé—´åŠ æƒæ¿€æ´»å€¼ | åœ¨è¾ƒä½å±‚æ•ˆæœæ›´å¥½ |
| **FullGrad** | è®¡ç®—æ‰€æœ‰åç½®çš„æ¢¯åº¦å¹¶æ±‚å’Œ | å®Œæ•´çš„æ¢¯åº¦ä¿¡æ¯ |
| **FEM** | æ¢¯åº¦æ— å…³æ–¹æ³•ï¼ŒäºŒå€¼åŒ–æ¿€æ´»å€¼ | å¿«é€Ÿï¼Œæ— éœ€æ¢¯åº¦ |
| **GradCAMElementWise** | é€å…ƒç´ ç›¸ä¹˜ååº”ç”¨ReLU | HiResCAMçš„å˜ä½“ |
| **KPCA-CAM** | ä½¿ç”¨æ ¸PCAä»£æ›¿PCA | EigenCAMçš„æ”¹è¿› |
| **ShapleyCAM** | ä½¿ç”¨æ¢¯åº¦å’ŒHessian-å‘é‡ç§¯ | åŸºäºShapleyå€¼ |
| **FinerCAM** | é€šè¿‡æ¯”è¾ƒç›¸ä¼¼ç±»åˆ«çªå‡ºå·®å¼‚ | ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ |

### 2. æ”¯æŒçš„æ¨¡å‹æ¶æ„

#### CNN æ¨¡å‹
- **ResNet** (18, 50ç­‰): `model.layer4[-1]`
- **VGG**: `model.features[-1]`
- **DenseNet**: `model.features[-1]`
- **MobileNet**: `model.features[-1]`
- **MNASNet**: `model.layers[-1]`

#### Vision Transformer æ¨¡å‹
- **ViT (Vision Transformer)**: `model.blocks[-1].norm1`
- **Swin Transformer**: `model.layers[-1].blocks[-1].norm1`
- **DeiT**: ç±»ä¼¼ViT

#### å¤šæ¨¡æ€æ¨¡å‹
- **CLIP**: `model.vision_model.encoder.layers[-1].layer_norm1`
  - æ”¯æŒå›¾åƒ-æ–‡æœ¬åŒ¹é…å¯è§†åŒ–
  - å¯ä»¥å¯è§†åŒ–æ¨¡å‹å¯¹ç‰¹å®šæ–‡æœ¬æè¿°çš„å…³æ³¨åŒºåŸŸ

#### å…¶ä»–æ¶æ„
- **Faster R-CNN**: `model.backbone`
- **YOLO**: æ”¯æŒç›®æ ‡æ£€æµ‹å¯è§†åŒ–
- **è¯­ä¹‰åˆ†å‰²æ¨¡å‹**: æ”¯æŒåƒç´ çº§å¯è§†åŒ–

### 3. åº”ç”¨åœºæ™¯

#### âœ… å›¾åƒåˆ†ç±»
- å¯è§†åŒ–æ¨¡å‹è®¤ä¸ºå›¾åƒå±äºæŸä¸ªç±»åˆ«çš„ä¾æ®
- ç†è§£æ¨¡å‹å…³æ³¨çš„å…³é”®ç‰¹å¾

#### âœ… ç›®æ ‡æ£€æµ‹
- å¯è§†åŒ–æ£€æµ‹æ¡†çš„ç”Ÿæˆä¾æ®
- ç†è§£æ¨¡å‹å¦‚ä½•å®šä½ç›®æ ‡

#### âœ… è¯­ä¹‰åˆ†å‰²
- åƒç´ çº§å¯è§†åŒ–
- ç†è§£åˆ†å‰²è¾¹ç•Œçš„ç”Ÿæˆ

#### âœ… å›¾åƒ-æ–‡æœ¬åŒ¹é… (CLIP)
- **ç»™å®šå›¾åƒå’Œæ–‡æœ¬ï¼Œå¯è§†åŒ–æ¨¡å‹åœ¨å›¾åƒä¸­å…³æ³¨ä¸æ–‡æœ¬æè¿°ç›¸å…³çš„åŒºåŸŸ**
- ä¾‹å¦‚ï¼šç»™å®šå›¾åƒå’Œæ–‡æœ¬"a dog"ï¼Œå¯è§†åŒ–æ¨¡å‹å…³æ³¨å›¾åƒä¸­ä¸"dog"ç›¸å…³çš„åŒºåŸŸ

#### âœ… åµŒå…¥ç›¸ä¼¼åº¦
- å¯è§†åŒ–å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§
- ç†è§£ç‰¹å¾ç©ºé—´ä¸­çš„å…³ç³»

### 4. å¹³æ»‘å’Œä¼˜åŒ–

- **aug_smooth**: æµ‹è¯•æ—¶æ•°æ®å¢å¼ºå¹³æ»‘ï¼ˆæ°´å¹³ç¿»è½¬ã€äº®åº¦è°ƒæ•´ï¼‰
- **eigen_smooth**: ä½¿ç”¨ç¬¬ä¸€ä¸»æˆåˆ†é™å™ª
- **æ‰¹å¤„ç†æ”¯æŒ**: æ‰€æœ‰æ–¹æ³•éƒ½æ”¯æŒæ‰¹é‡å›¾åƒå¤„ç†

### 5. è¯„ä¼°æŒ‡æ ‡

- **ROAD (Remove and Debias)**: æœ€å…ˆè¿›çš„å¯è§£é‡Šæ€§è¯„ä¼°æŒ‡æ ‡
- **ç½®ä¿¡åº¦å˜åŒ–**: æµ‹é‡ç§»é™¤é‡è¦åŒºåŸŸåçš„ç½®ä¿¡åº¦å˜åŒ–
- **å¤šå›¾åƒè¯„ä¼°**: æ”¯æŒæ‰¹é‡è¯„ä¼°

## ğŸ”§ CLIP æ¨¡å‹å¯è§†åŒ–

### åŠŸèƒ½è¯´æ˜

**æ˜¯çš„ï¼Œè¿™ä¸ªåº“å®Œå…¨æ”¯æŒ CLIP æ¨¡å‹çš„å¯è§†åŒ–ï¼**

CLIP æ¨¡å‹å¯è§†åŒ–å¯ä»¥ï¼š
- âœ… ç»™å®šå›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œè¾“å‡ºæ¨¡å‹åœ¨å›¾åƒä¸­å…³æ³¨ä¸æ–‡æœ¬ç›¸å…³çš„å†…å®¹
- âœ… æ”¯æŒå¤šä¸ªæ–‡æœ¬æ ‡ç­¾çš„æ¯”è¾ƒ
- âœ… å¯è§†åŒ–æ¨¡å‹å¦‚ä½•åŒ¹é…å›¾åƒå’Œæ–‡æœ¬

### ä½¿ç”¨æ–¹æ³•

1. **åŠ è½½ CLIP æ¨¡å‹**ï¼ˆæ”¯æŒæœ¬åœ°è·¯å¾„ï¼‰ï¼š
```python
from transformers import CLIPModel, CLIPProcessor

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
model_path = "/home/liying/Documents/clip-vit-large-patch14"
model = CLIPModel.from_pretrained(model_path)
processor = CLIPProcessor.from_pretrained(model_path)
```

2. **åˆ›å»ºåŒ…è£…ç±»**ï¼š
```python
class ImageClassifier(nn.Module):
    def __init__(self, labels, model_path):
        super().__init__()
        self.clip = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        self.labels = labels

    def forward(self, x):
        text_inputs = self.processor(text=self.labels, return_tensors="pt", padding=True)
        outputs = self.clip(
            pixel_values=x,
            input_ids=text_inputs['input_ids'].to(self.clip.device),
            attention_mask=text_inputs['attention_mask'].to(self.clip.device)
        )
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1)
        return probs
```

3. **é€‰æ‹©ç›®æ ‡å±‚**ï¼š
```python
target_layers = [model.clip.vision_model.encoder.layers[-1].layer_norm1]
```

4. **ä½¿ç”¨ reshape_transform**ï¼š
```python
def reshape_transform(tensor, height=16, width=16):
    # CLIPä½¿ç”¨patch size 14ï¼Œæ‰€ä»¥æ˜¯16x16 (224/14=16)
    result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))
    result = result.transpose(2, 3).transpose(1, 2)
    return result
```

5. **ç”Ÿæˆå¯è§†åŒ–**ï¼š
```python
with GradCAM(model=model, target_layers=target_layers,
             reshape_transform=reshape_transform) as cam:
    grayscale_cam = cam(input_tensor=input_tensor, targets=None)
    # targets=None ä¼šè‡ªåŠ¨é€‰æ‹©æœ€é«˜åˆ†çš„ç±»åˆ«
```

### ç¤ºä¾‹

åœ¨ `usage_examples/clip_example.py` ä¸­æœ‰å®Œæ•´çš„CLIPå¯è§†åŒ–ç¤ºä¾‹ã€‚

## ğŸ“ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨ï¼ˆResNet50ï¼‰

```python
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image
from torchvision.models import resnet50
import cv2
import numpy as np

model = resnet50(pretrained=True).eval()
target_layers = [model.layer4[-1]]

# åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
rgb_img = cv2.imread('image.jpg', 1)[:, :, ::-1]
rgb_img = np.float32(rgb_img) / 255
input_tensor = preprocess_image(rgb_img)

# æŒ‡å®šç›®æ ‡ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼š281 = "tabby cat"ï¼‰
targets = [ClassifierOutputTarget(281)]

# ç”ŸæˆCAM
with GradCAM(model=model, target_layers=target_layers) as cam:
    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)
    grayscale_cam = grayscale_cam[0, :]
    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)
    cv2.imwrite('cam_output.jpg', cam_image)
```

## ğŸ¨ è¾“å‡ºç»“æœ

åº“ä¼šç”Ÿæˆå¤šç§å¯è§†åŒ–ç»“æœï¼š
1. **çƒ­åŠ›å›¾ (Heatmap)**: åŸå§‹CAMçƒ­åŠ›å›¾
2. **å åŠ å›¾ (CAM on Image)**: CAMå åŠ åœ¨åŸå›¾ä¸Š
3. **Guided Backpropagation**: å¼•å¯¼åå‘ä¼ æ’­ç»“æœ
4. **ç»„åˆå›¾ (CAM + GB)**: ä¸¤ç§æ–¹æ³•çš„ç»„åˆ

## ğŸ“š æ›´å¤šèµ„æº

- å®˜æ–¹æ–‡æ¡£: https://jacobgil.github.io/pytorch-gradcam-book
- GitHub: https://github.com/jacobgil/pytorch-grad-cam
- æ•™ç¨‹æ–‡ä»¶å¤¹: `tutorials/`
- ä½¿ç”¨ç¤ºä¾‹: `usage_examples/`

## ğŸ’¡ å…³é”®è¦ç‚¹

1. **é€‰æ‹©æ­£ç¡®çš„ç›®æ ‡å±‚**ï¼šä¸åŒæ¶æ„éœ€è¦ä¸åŒçš„å±‚
2. **ä½¿ç”¨ reshape_transform**ï¼šå¯¹äºTransformeræ¶æ„ï¼ˆViT, CLIPç­‰ï¼‰æ˜¯å¿…éœ€çš„
3. **é€‰æ‹©åˆé€‚çš„targets**ï¼šå¯ä»¥æŒ‡å®šç‰¹å®šç±»åˆ«ï¼Œæˆ–ä½¿ç”¨Noneè‡ªåŠ¨é€‰æ‹©æœ€é«˜åˆ†
4. **å¹³æ»‘é€‰é¡¹**ï¼šä½¿ç”¨ `aug_smooth` å’Œ `eigen_smooth` å¯ä»¥è·å¾—æ›´å¥½çš„è§†è§‰æ•ˆæœ
5. **æ‰¹å¤„ç†**ï¼šæ‰€æœ‰æ–¹æ³•éƒ½æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œæé«˜æ•ˆç‡
